I"ó+<p>In the previous section, we have learned how to represent a graph using ‚Äúshallow‚Äù encoders. Those techniques give us a powerful expression over a graph in a vector space, but there are limitations. In this section, we will explore several different approaches using deep encoders.</p>

<h2 id="limitations-of-shallow-encoders">Limitations of ‚ÄúShallow Encoders‚Äù</h2>
<ul>
  <li>Scalability: every node has its own embeddings</li>
  <li>Inherently Transductive: it cannot generate embeddings for unseen nodes.</li>
  <li>Node Feature Excluded: it cannot leverage node features.</li>
  <li>Not Task Specific: it cannot be generalized to train with different loss function.</li>
</ul>

<p>Fortunately, the above limitations can be solved by using graph neural networks.</p>

<h2 id="graph-convolutional-networks-gcn">Graph Convolutional Networks (GCN)</h2>

<p>Traditionally, neural network are designed for fixed-sized graphs. For example, we could consider a image as fixed-size graph or text as a line graph. However, most of the graphs in the real world has arbitrary size and complex topological structure. Therefore, we need to define the computation graph of GCN differently.</p>

<h3 id="setup">Setup</h3>
<p>Given <script type="math/tex">G = (V, A, X)</script> be a graph such that:</p>
<ul>
  <li><script type="math/tex">V</script> is the vertex set</li>
  <li><script type="math/tex">A</script> is the adjacency matrix</li>
  <li><script type="math/tex">X\in \mathbb{R}^{m\times\rvert V \rvert}</script> is the node feature matrix</li>
</ul>

<h3 id="computation-graph-and-generalized-convolution">Computation Graph and Generalized Convolution</h3>
<p><img src="../assets/img/aggregate_neighbors.png?style=centerme" alt="aggregate_neighbors" />
Suppose <script type="math/tex">G</script> is the graph in the above figure on the left, our goal is to define a computation graph of GCN with convolution. The GCN should keep the structure of the graph and incorporate the neighboring features. For example, if we want to create an embedding for node <script type="math/tex">A</script>, we can aggregate the information from its neighbour: <script type="math/tex">B, C, D</script>.
The aggregation (little boxes) needs to be <strong>order invariant</strong> (max, average, etc.). 
The computation graph for all the nodes in the graph with two layers deep will look like the following:
<img src="../assets/img/computation_graph.png?style=centerme" alt="computation_graph" />
Notice that every node defines a computation graph based on its neighbors. In particular, the computation graph for node <script type="math/tex">A</script> can be viewed as the following:
<img src="../assets/img/computation_graph_for_a.png?style=centerme" alt="computation_graph_for_a" />
Layer-0 is the input layer with node feature <script type="math/tex">X</script>. In each layer, GCN combines the node features and create some hidden representation.</p>

<h3 id="deep-encoders">Deep Encoders</h3>
<p>With the above idea, here is the mathematical expression at each layer using the average aggregation function:</p>
<ul>
  <li>at 0th layer: <script type="math/tex">h^0_v = x_v</script>, this is the node feature</li>
  <li>at kth layer: <script type="math/tex">h_v^{K} = \sigma(W_k\sum_{u\in N(v)}\frac{h_u^{k-1}}{\rvert N(v)\rvert} + B_kh_v^{k-1}), \forall k \in \{1, .., K\}</script>,</li>
</ul>

<p><script type="math/tex">h_v^{k-1}</script> is the embedding from the previous layer, <script type="math/tex">\sigma</script> is the activation function (e.g. ReLU), <script type="math/tex">W_k, B_k</script> are the trainable parameters, and <script type="math/tex">\rvert N(v) \rvert</script> are the neighbours of node <script type="math/tex">v</script>.</p>

<ul>
  <li><script type="math/tex">z_v = h_v^{k}</script>, this is the final embedding for after <script type="math/tex">k</script> layers</li>
</ul>

<p>Equivalently, these computation can be written in a vector: <script type="math/tex">H^{l+1} = \sigma(H^{l}W_0^{l} + \tilde{A}H^{l}W_1^{l})</script> such that <script type="math/tex">\tilde{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script></p>

<h3 id="training-the-model">Training the Model</h3>
<p>We can feed these embeddings into any loss functions and run stochastic gradient descent to train the weight parameters.
For example, for a binary classification task, we can define the loss function as:</p>

<script type="math/tex; mode=display">L = \sum_{v\in V} y_v \log(\sigma(z_v^T\theta)) + (1-y_v)\log(1-\sigma(z_v^T\theta))</script>

<p>Here, <script type="math/tex">y_v</script> is the node class label, <script type="math/tex">z_v</script> is the encoder output, <script type="math/tex">\theta</script> is the classification weight, and <script type="math/tex">\sigma</script> can be the sigmoid function.
In addition, we can also train the model in an unsupervised manner by using: random walk, graph factorization, node proximity, etc.</p>

<h3 id="inductive-capability">Inductive Capability</h3>
<p>GCN can also be generalized to unseen nodes in a graph. For example, if the model is trained using nodes <script type="math/tex">A, B, C</script>, the newly added nodes <script type="math/tex">D,E,F</script> can also be evaluated since all the parameters are share crossed all nodes.
<img src="../assets/img/apply_to_new_nodes.png?style=centerme" alt="apply_to_new_nodes" /></p>

<h2 id="graphsage">GraphSage</h2>
<p>So far we have explored a simple neighbourhood aggregation methods, but we can also generalize the aggregations in the following form:</p>

<script type="math/tex; mode=display">h_v^{K} = \sigma([W_k AGG(\{h_u^{k-1}, \forall u \in N(v)\}), B_kh_v^{k-1}])</script>

<p>For a node <script type="math/tex">v</script>, we can apply different aggregation methods to the neighbors using other aggregation functions (<script type="math/tex">AGG</script>), then concatenating the features with the target node itself.
Here are some commonly used aggregation functions:</p>
<ul>
  <li>Mean: Take a weighted average of neighbors</li>
</ul>

<script type="math/tex; mode=display">AGG = \sum_{u\in N(v)} \frac{h_u^{k-1}}{\rvert N(v) \rvert}</script>

<ul>
  <li>Pool: Transform neighbor vectors and apply symmetric vector function (<script type="math/tex">\gamma</script> can be element-wise mean or max)</li>
</ul>

<script type="math/tex; mode=display">AGG = \gamma(\{ Qh_u^{k-1}, \forall u\in N(v)\})</script>

<ul>
  <li>LSTM: Apply LSTM to reshuffled neighbors</li>
</ul>

<script type="math/tex; mode=display">AGG = LSTM(\{ h_u^{k-1}, \forall u\in \pi(N(v)\}))</script>

<h2 id="attention">Attention</h2>
<p>What happened if we want to incorporate different weights for different nodes? Maybe some nodes can express more important information than others.</p>

<p>Let <script type="math/tex">\alpha_{vu}</script> be the weighting factor (importance) of node <script type="math/tex">u</script>‚Äôs message to node <script type="math/tex">v</script>. Previously, in the average aggregation, we defined <script type="math/tex">\alpha=\frac{1}{\rvert N(v) \rvert}</script>, but we can also explicitly define <script type="math/tex">\alpha</script> based on the structural property of a graph.</p>

<h3 id="attention-mechanism">Attention Mechanism</h3>
<p>Let <script type="math/tex">\alpha_{uv}</script> be computed as a byproduct of an attention mechanism <script type="math/tex">a</script>.
Let <script type="math/tex">a</script> compute attention coefficients <script type="math/tex">e_{vu}</script> across pairs of nodes <script type="math/tex">u, v</script> based on their messages. Then, we have the following:</p>

<script type="math/tex; mode=display">e_{vu} = a(W_kh_u^{k-1}, W_kh)v^{k-1}</script>

<p>where <script type="math/tex">e_{vu}</script> indicates the importance of node <script type="math/tex">u</script>‚Äôs message to node <script type="math/tex">v</script>. Then, normalize the coefficients using the softmax function in
order to compare importance across different neighborhoods:</p>

<script type="math/tex; mode=display">\alpha_{vu} = \frac{\exp(e_{vu})}{\sum_{k\in N(v)}\exp(e_{vk})}</script>

<p>Therefore, we have:</p>

<script type="math/tex; mode=display">h_{v}^k = \sigma(\sum_{u\in N(v)}\alpha_{vu}W_kh^{k-1}_u)</script>

<p>Notice that this approach is agnostic to the choice of <script type="math/tex">a</script> and parameters of <script type="math/tex">a</script> are trained jointly.</p>

<h2 id="reference">Reference</h2>
<p>Here is a list of useful references:</p>

<p><strong>Tutorials and Overview:</strong></p>
<ul>
  <li><a href="https://arxiv.org/pdf/1806.01261.pdf">Relational inductive biases and graph networks (Battaglia et al., 2018)</a></li>
  <li><a href="https://arxiv.org/pdf/1709.05584.pdf">Representation learning on graphs: Methods and applications (Hamilton et al., 2017)</a></li>
</ul>

<p><strong>Attention-based Neighborhood Aggregation:</strong></p>
<ul>
  <li><a href="https://arxiv.org/pdf/1710.10903.pdf">Graph attention networks (Hoshen, 2017; Velickovic et al., 2018; Liu et al., 2018)</a></li>
</ul>

<p><strong>Embedding the Entire Graphs:</strong></p>
<ul>
  <li>Graph neural nets with edge embeddings (<a href="https://arxiv.org/pdf/1806.01261.pdf">Battaglia et al., 2016</a>; <a href="https://arxiv.org/pdf/1704.01212.pdf">Gilmer et. al., 2017</a>)</li>
  <li>Embedding entire graphs (<a href="https://dl.acm.org/citation.cfm?id=2969488">Duvenaud et al., 2015</a>; <a href="https://arxiv.org/pdf/1603.05629.pdf">Dai et al., 2016</a>; <a href="https://arxiv.org/abs/1803.03324">Li et al., 2018</a>) and graph pooling
(<a href="https://arxiv.org/pdf/1806.08804.pdf">Ying et al., 2018</a>, <a href="https://arxiv.org/pdf/1911.05954.pdf">Zhang et al., 2018</a>)</li>
  <li><a href="https://arxiv.org/pdf/1802.08773.pdf">Graph generation</a> and <a href="https://arxiv.org/pdf/1802.04687.pdf">relational inference</a> (You et al., 2018; Kipf et al., 2018)</li>
  <li><a href="https://arxiv.org/pdf/1810.00826.pdf">How powerful are graph neural networks(Xu et al., 2017)</a></li>
</ul>

<p><strong>Embedding Nodes:</strong></p>
<ul>
  <li>Varying neighborhood: <a href="https://arxiv.org/pdf/1806.03536.pdf">Jumping knowledge networks Xu et al., 2018)</a>, <a href="https://arxiv.org/pdf/1802.00910.pdf">GeniePath (Liu et al., 2018</a></li>
  <li><a href="https://arxiv.org/pdf/1906.04817.pdf">Position-aware GNN (You et al. 2019)</a></li>
</ul>

<p><strong>Spectral Approaches to Graph Neural Networks:</strong></p>
<ul>
  <li><a href="https://arxiv.org/pdf/1606.09375.pdf">Spectral graph CNN</a> &amp; <a href="https://arxiv.org/pdf/1609.02907.pdf">ChebNet</a> [Bruna et al., 2015; Defferrard et al., 2016)</li>
  <li><a href="https://arxiv.org/pdf/1611.08097.pdf">Geometric deep learning (Bronstein et al., 2017; Monti et al., 2017)</a></li>
</ul>

<p><strong>Other GNN Techniques:</strong></p>
<ul>
  <li><a href="https://arxiv.org/pdf/1905.12265.pdf">Pre-training Graph Neural Networks (Hu et al., 2019)</a></li>
  <li><a href="https://arxiv.org/pdf/1903.03894.pdf">GNNExplainer: Generating Explanations for Graph Neural Networks (Ying et al., 2019)</a></li>
</ul>

:ET