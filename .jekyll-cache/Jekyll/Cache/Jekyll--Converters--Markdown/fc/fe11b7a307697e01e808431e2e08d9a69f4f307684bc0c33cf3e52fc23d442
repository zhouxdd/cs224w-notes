I"u<<p>Here we study the important class of spectral methods for understanding networks on a global level. By “spectral” we mean the spectrum, or eigenvalues, of matrices derived from graphs, which will give us insight into the structure of the graphs themselves. In particular, we will explore spectral clustering algorithms, which take advantage of these tools for clustering nodes in graphs.</p>

<p>The spectral clustering algorithms we will explore generally consist of three basic stages.</p>
<ol>
  <li>Preprocessing: construct a matrix representation of a graph, such as the adjacency matrix (but we will explore other options)</li>
  <li>Decomposition: compute the eigenvectors and eigenvalues of the matrix, and use these to create a low-dimensional representation space</li>
  <li>Grouping: assign points to clusters based on their representation in this space</li>
</ol>

<h1 id="graph-partitioning">Graph Partitioning</h1>
<p>Let’s formalize the task we would like to solve. We start out with an undirected graph <script type="math/tex">G(V, E)</script>. Our goal is to partition <script type="math/tex">V</script> into two disjoint groups <script type="math/tex">A, B</script> (so <script type="math/tex">A \cap B = \emptyset</script> and <script type="math/tex">A \cup B = V</script>) in a way that maximizes the number of connections internal to the groups and minimizes the number of connections between the two groups.</p>

<p>To further formalize the objective, let’s introduce some terminology:</p>
<ul>
  <li>Cut: how much connection there is between two disjoint sets of nodes. <script type="math/tex">cut(A, B) = \sum_{i \in A, j \in B} w_{ij}</script> where <script type="math/tex">w_{ij}</script> is the weight of the edge between nodes <script type="math/tex">i</script> and <script type="math/tex">j</script>.</li>
  <li>Minimum cut: <script type="math/tex">\arg \min_{A, B} cut(A, B)</script></li>
</ul>

<p>Since we want to minimize the number of connections between <script type="math/tex">A</script> and <script type="math/tex">B</script>, we might decide to make the minimum cut our objective. However, we find that we end up with very unintuitive clusters this way – we can often simply set <script type="math/tex">A</script> to be a single node with very few outgoing connections, and <script type="math/tex">B</script> to be the rest of the network, to get a very small cut. What we need is a measure that also considers internal cluster connectivity.</p>

<p>Enter the <strong>conductance</strong>, which balances between-group and within-group connectivity concerns. We define <script type="math/tex">\phi(A, B) = \frac{cut(A, B)}{min(vol(A), vol(B))}</script> where <script type="math/tex">vol(A) = \sum_{i \in A} k_i</script>, the total (weighted) degree of the nodes in <script type="math/tex">A</script>. We can roughly think of conductance as analogous to a surface area to volume ratio: the numerator is the area of the shared surface between <script type="math/tex">A</script> and <script type="math/tex">B</script>, and the denominator measures volume while trying to ensure <script type="math/tex">A</script> and <script type="math/tex">B</script> have similar volumes. Because of this nuanced measure, picking <script type="math/tex">A</script> and <script type="math/tex">B</script> to minimize the conductance results in more balanced partitions than minimizing the cut. The challenge then becomes to efficiently find a good partition, since minimizing conductance is NP-hard.</p>

<h1 id="spectral-graph-partitioning">Spectral Graph Partitioning</h1>
<p>Enter spectral graph partitioning, a method that will allow us to pin down the conductance using eigenvectors. We’ll start by introducing some basic techniques in spectral graph theory.</p>

<p>The goal of spectral graph theory is to analyze the “spectrum” of matrices representing graphs. By spectrum we mean the set <script type="math/tex">\Lambda = \{\lambda_1, \ldots, \lambda_n\}</script> of eigenvalues <script type="math/tex">\lambda_i</script> of a matrix representing a graph, in order of their magnitudes, along with their corresponding eigenvalues. For example, the largest eigenvector/eigenvalue pair for the adjacency matrix of a d-regular graph is the all-ones vector <script type="math/tex">x = (1, 1, \ldots, 1)</script>, with eigenvalue <script type="math/tex">\lambda = d</script>. Exercise: what are some eigenvectors for a disconnected graph with two components, each component d-regular? Note that by the spectral theorem, the adjacency matrix (which is real and symmetric) has a complete spectrum of orthogonal eigenvectors.</p>

<p>What kinds of matrices can we analyze using spectral graph theory?</p>
<ol>
  <li>The adjacency matrix: this matrix is a good starting point due to its direct relation to graph structure. It also has the important property of being symmetric, which means that it has a complete spectrum of real-valued, orthogonal eigenvectors.</li>
  <li>Laplacian matrix <script type="math/tex">L</script>: it’s defined by <script type="math/tex">L = D - A</script> where <script type="math/tex">D</script> is a diagonal matrix such that <script type="math/tex">D_{ii}</script> equals the degree of node <script type="math/tex">i</script> and <script type="math/tex">A</script> is the adjacency matrix of the graph. The Laplacian takes us farther from the direct structure of a graph, but has some interesting properties which will take us towards deeper structural aspects of our graph. We note that the all-ones vector is an eigenvector of the Laplacian with eigenvalue 0. In addition, since <script type="math/tex">L</script> is symmetric, it has a complete spectrum of real-valued, orthogonal eigenvectors. Finally, <script type="math/tex">L</script> is positive-semidefinite, which means it has three equivalent properties: its eigenvalues are all non-negative, <script type="math/tex">L = N^T N</script> for some matrix <script type="math/tex">N</script> and <script type="math/tex">x^T Lx \geq 0</script> for every vector <script type="math/tex">x</script>. This property allows us to use new linear algebra tools to understand <script type="math/tex">L</script> and thus our original graph.</li>
</ol>

<p>In particular, <script type="math/tex">\lambda_2</script>, the second smallest eigenvalue of <script type="math/tex">L</script>, is already fascinating and studying it will let us make big strides in understanding graph clustering. By the theory of Rayleigh quotients, we have that <script type="math/tex">\lambda_2 = \min_{x: x^T w_1 = 0} \frac{x^T L x}{x^T x}</script> where <script type="math/tex">w_1</script> is the eigenvector corresponding to eigenvalue <script type="math/tex">\lambda_1</script>; in other words, we minimize the objective in the subspace of vectors orthogonal to the first eigenvector in order to find the second eigenvector (remember that <script type="math/tex">L</script> is symmetric and thus has an orthogonal basis of eigenvalues). On a high level, Rayleigh quotients frame the eigenvector search as an optimization problem, letting us bring optimization techniques to bear. Note that the objective value does not depend on the magnitude of <script type="math/tex">x</script>, so we can constrain its magnitude to be 1. Note additionally that we know that the first eigenvector of <script type="math/tex">L</script> is the all-ones vector with eigenvalue 0, so saying that <script type="math/tex">x</script> is orthogonal to this vector is equivalent to saying that <script type="math/tex">\sum_i x_i = 0</script>.</p>

<p>Using these properties and the definition of <script type="math/tex">L</script>, we can write out a more concrete formula for <script type="math/tex">\lambda_2</script>: <script type="math/tex">\lambda_2 = \min_x \frac{\sum_{(i, j) \in E} (x_i - x_j)^2}{\sum_i x_i^2}</script>, subject to the constraint <script type="math/tex">\sum_i x_i = 0</script>. If we additionally constrain <script type="math/tex">x</script> to have unit length, the objective turns into simply <script type="math/tex">\min_x \sum_{(i, j) \in E} (x_i - x_j)^2</script>.</p>

<p>How does <script type="math/tex">\lambda_2</script> relate to our original objective of finding a best partition of our graph? Let’s express our partition <script type="math/tex">(A, B)</script> as a vector <script type="math/tex">y</script> defined by <script type="math/tex">y_i = 1</script> if <script type="math/tex">i \in A</script> and <script type="math/tex">y_i = -1</script> if <script type="math/tex">i \in B</script>. Instead of using the conductance here, let’s first try to minimize the cut while taking care of the problem of balancing partition sizes by enforcing that <script type="math/tex">\vert A\vert = \vert B\vert</script> (balance size of partitions), which amounts to constraining <script type="math/tex">\sum_i y_i = 0</script>. Given this size constraint, let’s minimize the cut of the partition, i.e. find <script type="math/tex">y</script> that minimizes <script type="math/tex">\sum_{(i, j) \in E} (y_i - y_j)^2</script>. Note that the entries of <script type="math/tex">y</script> must be <script type="math/tex">+1</script> or <script type="math/tex">-1</script>, which has the consequence that the length of <script type="math/tex">y</script> is fixed. <em>This optimization problem looks a lot like the definition of <script type="math/tex">\lambda_2</script>!</em> Indeed, by our findings above we have that this objective is minimized by <script type="math/tex">\lambda_2</script> of our Laplacian, and the optimal clustering <script type="math/tex">y</script> is given by its corresponding eigenvector, known as the <strong>Fiedler vector</strong>.</p>

<p>Now that we have a link between an eigenvalue of <script type="math/tex">L</script> and graph partitioning, let’s push the connection further and see if we can get rid of the hard <script type="math/tex">\vert A\vert = \vert B\vert</script> constraint – maybe there is a link between the more flexible conductance measure and <script type="math/tex">\lambda_2</script>. Let’s rephrase conductance here in the following way: if a graph <script type="math/tex">G</script> is partitioned into <script type="math/tex">A</script> and <script type="math/tex">B</script> where <script type="math/tex">\vert A\vert \leq \vert B\vert</script>, then the conductance of the cut is defined as <script type="math/tex">\beta = cut(A, B)/\vert A\vert</script>. A result called the Cheeger inequality links <script type="math/tex">\beta</script> to <script type="math/tex">\lambda_2</script>: in particular, <script type="math/tex">\frac{\beta^2}{2k_{max}} \leq \lambda_2 \leq 2\beta</script> where <script type="math/tex">k_{max}</script> is the maximum node degree in the graph. The upper bound on <script type="math/tex">\lambda_2</script> is most useful to us for graph partitioning, since we are trying to minimize the conductance; it says that <script type="math/tex">\lambda_2</script> gives us a good estimate of the conductance – we never overestimate it more than by a factor of 2! The corresponding eigenvector <script type="math/tex">x</script> is defined by <script type="math/tex">x_i = -1/a</script> if <script type="math/tex">i \in A</script> and <script type="math/tex">x_j = 1/b</script> if <script type="math/tex">i \in B</script>; the signs of the entries of <script type="math/tex">x</script> give us the partition assignments of each node.</p>

<h1 id="spectral-partitioning-algorithm">Spectral Partitioning Algorithm</h1>
<p>Let’s put all our findings together to state the spectral partitioning algorithm.</p>
<ol>
  <li>Preprocessing: build the Laplacian matrix <script type="math/tex">L</script> of the graph</li>
  <li>Decomposition: map vertices to their corresponding entries in the second eigenvector</li>
  <li>Grouping: sort these entries and split the list in two to arrive at a graph partition</li>
</ol>

<p>Some practical considerations emerge.</p>
<ul>
  <li>How do we choose a splitting point in step 3? There’s flexibility here – we can use simple approaches like splitting at zero or the median value, or more expensive approaches like minimizing the normalized cut in one dimension.</li>
  <li>How do we partition a graph into more than two clusters? We could divide the graph into two clusters, then further subdivide those clusters, etc (Hagen et al ‘92)…but that can be inefficient and unstable. Instead, we can cluster using multiple eigenvectors, letting each node be represented by its component in these eigenvectors, then cluster these representations, e.g. through k-means (Shi-Malik ‘00), which is commonly used in recent papers. This method is also more principled in the sense that it approximates the optimal k-way normalized cut, emphasizes cohesive clusters and maps points to a well-separated embedded space. Furthermore, using an eigenvector basis ensures that less information is lost, since we can choose to keep the (more informative) components corresponding to bigger eigenvalues.</li>
  <li>How do we select the number of clusters? We can try to pick the number of clusters <script type="math/tex">k</script> to maximize the <strong>eigengap</strong>, the absolute difference between two consecutive eigenvalues (ordered by descending magnitude).</li>
</ul>

<h1 id="motif-based-spectral-clustering">Motif-Based Spectral Clustering</h1>
<p>What if we want to cluster by higher-level patterns than raw edges? We can instead cluster graph motifs into “modules”. We can do everything in an analogous way. Let’s start by proposing analogous definitions for cut, volume and conductance:</p>
<ul>
  <li><script type="math/tex">cut_M(S)</script> is the number of motifs for which some nodes in the motif are in one side of the cut and the rest of the nodes are in the other cut</li>
  <li><script type="math/tex">vol_M(S)</script> is the number of motif endpoints in <script type="math/tex">S</script> for the motif <script type="math/tex">M</script></li>
  <li>We define <script type="math/tex">\phi(S) = cut_M(S) / vol_M(S)</script></li>
</ul>

<p>How do we find clusters of motifs? Given a motif <script type="math/tex">M</script> and graph <script type="math/tex">G</script>, we’d like to find a set of nodes <script type="math/tex">S</script> that minimizes <script type="math/tex">\phi_M(S)</script>. This problem is NP-hard, so we will again make use of spectral methods, namely <strong>motif spectral clustering</strong>:</p>
<ol>
  <li>Preprocessing: create a matrix <script type="math/tex">W^{(M)}</script> defined by <script type="math/tex">W_{ij}^{(M)}</script> equals the number of times edge <script type="math/tex">(i, j)</script> participates in <script type="math/tex">M</script>.</li>
  <li>Decomposition: use standard spectral clustering on <script type="math/tex">W^{(M)}</script>.</li>
  <li>Grouping: same as standard spectral clustering</li>
</ol>

<p>Again, we can prove a motif version of the Cheeger inequality to show that the motif conductance found by our algorithm is bounded above by <script type="math/tex">4\sqrt{\phi_M^*}</script>, where <script type="math/tex">\phi_M^*</script> is the optimal conductance.</p>

<p>We can apply this method to cluster the food web (which has motifs dictated by biology) and gene regulatory networks (in which directed, signed triads play an important role).</p>
:ET